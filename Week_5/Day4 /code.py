# import random


# import nltk
# with open('data.txt', 'r') as file:
#     text = in_file.read(data.txt)
#     sents = nltk.sent_tokenize(text)

# import re
# file = open("data.txt", "r")
# doclist = [ line for line in file ]
# docstr = '' . join(doclist)
# sentences = re.split(r'[.!?]', docstr)

# print (sentences)
# inputx = input("how long should your sentence be in numbers")

# def get_words_from_file():
# 	my_words = open("data.txt")
# 	print (my_words.read())
# 	print (my_words) 
	

#print(get_words_from_file())



#for i in range(10000):
	#Person(myfaker.first_name(), randint(1,85), myfaker.country()).save()

#for i in range(inputx):
#	pass

# print(get_words_from_file)
# print(random.choice(my_words))



# wli= [] #word list 

# for i in f: 
# 	l = l.split ()

# 	for w in l: 
# 		wli.append(w)

# wli.sort()

# wd= {}

# for w in wli:
# 	wd[w] = wli.count(w)